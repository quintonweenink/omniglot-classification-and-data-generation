{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "963"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.datasets import Omniglot\n",
    "omniglot = Omniglot(root=\"./data\", download=True)\n",
    "omniglot.download()\n",
    "\n",
    "alldata = []\n",
    "for i in omniglot:\n",
    "    alldata.append(i[1])\n",
    "    \n",
    "max(alldata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All images are have 105 by 105 pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<PIL.Image.Image image mode=L size=105x105 at 0x7FCB9D46CA20>, 15)\n"
     ]
    }
   ],
   "source": [
    "image = omniglot.__getitem__(300)\n",
    "print(image)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "imgplot = plt.imshow(image[0])\n",
    "# image[0].getdata()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot a histogram of the image pixel count to see the range of pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  469.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0., 10556.]),\n",
       " array([  0.        ,   0.99609375,   1.9921875 ,   2.98828125,\n",
       "          3.984375  ,   4.98046875,   5.9765625 ,   6.97265625,\n",
       "          7.96875   ,   8.96484375,   9.9609375 ,  10.95703125,\n",
       "         11.953125  ,  12.94921875,  13.9453125 ,  14.94140625,\n",
       "         15.9375    ,  16.93359375,  17.9296875 ,  18.92578125,\n",
       "         19.921875  ,  20.91796875,  21.9140625 ,  22.91015625,\n",
       "         23.90625   ,  24.90234375,  25.8984375 ,  26.89453125,\n",
       "         27.890625  ,  28.88671875,  29.8828125 ,  30.87890625,\n",
       "         31.875     ,  32.87109375,  33.8671875 ,  34.86328125,\n",
       "         35.859375  ,  36.85546875,  37.8515625 ,  38.84765625,\n",
       "         39.84375   ,  40.83984375,  41.8359375 ,  42.83203125,\n",
       "         43.828125  ,  44.82421875,  45.8203125 ,  46.81640625,\n",
       "         47.8125    ,  48.80859375,  49.8046875 ,  50.80078125,\n",
       "         51.796875  ,  52.79296875,  53.7890625 ,  54.78515625,\n",
       "         55.78125   ,  56.77734375,  57.7734375 ,  58.76953125,\n",
       "         59.765625  ,  60.76171875,  61.7578125 ,  62.75390625,\n",
       "         63.75      ,  64.74609375,  65.7421875 ,  66.73828125,\n",
       "         67.734375  ,  68.73046875,  69.7265625 ,  70.72265625,\n",
       "         71.71875   ,  72.71484375,  73.7109375 ,  74.70703125,\n",
       "         75.703125  ,  76.69921875,  77.6953125 ,  78.69140625,\n",
       "         79.6875    ,  80.68359375,  81.6796875 ,  82.67578125,\n",
       "         83.671875  ,  84.66796875,  85.6640625 ,  86.66015625,\n",
       "         87.65625   ,  88.65234375,  89.6484375 ,  90.64453125,\n",
       "         91.640625  ,  92.63671875,  93.6328125 ,  94.62890625,\n",
       "         95.625     ,  96.62109375,  97.6171875 ,  98.61328125,\n",
       "         99.609375  , 100.60546875, 101.6015625 , 102.59765625,\n",
       "        103.59375   , 104.58984375, 105.5859375 , 106.58203125,\n",
       "        107.578125  , 108.57421875, 109.5703125 , 110.56640625,\n",
       "        111.5625    , 112.55859375, 113.5546875 , 114.55078125,\n",
       "        115.546875  , 116.54296875, 117.5390625 , 118.53515625,\n",
       "        119.53125   , 120.52734375, 121.5234375 , 122.51953125,\n",
       "        123.515625  , 124.51171875, 125.5078125 , 126.50390625,\n",
       "        127.5       , 128.49609375, 129.4921875 , 130.48828125,\n",
       "        131.484375  , 132.48046875, 133.4765625 , 134.47265625,\n",
       "        135.46875   , 136.46484375, 137.4609375 , 138.45703125,\n",
       "        139.453125  , 140.44921875, 141.4453125 , 142.44140625,\n",
       "        143.4375    , 144.43359375, 145.4296875 , 146.42578125,\n",
       "        147.421875  , 148.41796875, 149.4140625 , 150.41015625,\n",
       "        151.40625   , 152.40234375, 153.3984375 , 154.39453125,\n",
       "        155.390625  , 156.38671875, 157.3828125 , 158.37890625,\n",
       "        159.375     , 160.37109375, 161.3671875 , 162.36328125,\n",
       "        163.359375  , 164.35546875, 165.3515625 , 166.34765625,\n",
       "        167.34375   , 168.33984375, 169.3359375 , 170.33203125,\n",
       "        171.328125  , 172.32421875, 173.3203125 , 174.31640625,\n",
       "        175.3125    , 176.30859375, 177.3046875 , 178.30078125,\n",
       "        179.296875  , 180.29296875, 181.2890625 , 182.28515625,\n",
       "        183.28125   , 184.27734375, 185.2734375 , 186.26953125,\n",
       "        187.265625  , 188.26171875, 189.2578125 , 190.25390625,\n",
       "        191.25      , 192.24609375, 193.2421875 , 194.23828125,\n",
       "        195.234375  , 196.23046875, 197.2265625 , 198.22265625,\n",
       "        199.21875   , 200.21484375, 201.2109375 , 202.20703125,\n",
       "        203.203125  , 204.19921875, 205.1953125 , 206.19140625,\n",
       "        207.1875    , 208.18359375, 209.1796875 , 210.17578125,\n",
       "        211.171875  , 212.16796875, 213.1640625 , 214.16015625,\n",
       "        215.15625   , 216.15234375, 217.1484375 , 218.14453125,\n",
       "        219.140625  , 220.13671875, 221.1328125 , 222.12890625,\n",
       "        223.125     , 224.12109375, 225.1171875 , 226.11328125,\n",
       "        227.109375  , 228.10546875, 229.1015625 , 230.09765625,\n",
       "        231.09375   , 232.08984375, 233.0859375 , 234.08203125,\n",
       "        235.078125  , 236.07421875, 237.0703125 , 238.06640625,\n",
       "        239.0625    , 240.05859375, 241.0546875 , 242.05078125,\n",
       "        243.046875  , 244.04296875, 245.0390625 , 246.03515625,\n",
       "        247.03125   , 248.02734375, 249.0234375 , 250.01953125,\n",
       "        251.015625  , 252.01171875, 253.0078125 , 254.00390625,\n",
       "        255.        ]),\n",
       " <a list of 256 Patch objects>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(image[0].getdata(), bins=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "from myomniglot import MyOmniglot\n",
    "\n",
    "setname = 'images_background_small1'\n",
    "\n",
    "dataset = MyOmniglot(root='./data', setname=setname, transform=transform)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qweenink/.local/lib/python3.5/site-packages/ipykernel_launcher.py:49: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class Net(nn.Module):    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "          \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "          \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p = 0.5),\n",
    "            nn.Linear(43264, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p = 0.5),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p = 0.5),\n",
    "            nn.Linear(512, 136),\n",
    "        )\n",
    "          \n",
    "        for m in self.features.children():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "        \n",
    "        for m in self.classifier.children():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "                \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x   \n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] loss: 5.573\n",
      "[1,    11] loss: 5.432\n",
      "[1,    21] loss: 5.267\n",
      "[2,     1] loss: 4.989\n",
      "[2,    11] loss: 4.907\n",
      "[2,    21] loss: 4.775\n",
      "[3,     1] loss: 4.824\n",
      "[3,    11] loss: 4.568\n",
      "[3,    21] loss: 4.580\n",
      "[4,     1] loss: 4.347\n",
      "[4,    11] loss: 4.082\n",
      "[4,    21] loss: 4.210\n",
      "[5,     1] loss: 4.097\n",
      "[5,    11] loss: 4.128\n",
      "[5,    21] loss: 4.148\n",
      "[6,     1] loss: 4.037\n",
      "[6,    11] loss: 3.763\n",
      "[6,    21] loss: 3.797\n",
      "[7,     1] loss: 3.568\n",
      "[7,    11] loss: 3.705\n",
      "[7,    21] loss: 3.583\n",
      "[8,     1] loss: 3.426\n",
      "[8,    11] loss: 3.534\n",
      "[8,    21] loss: 3.385\n",
      "[9,     1] loss: 3.428\n",
      "[9,    11] loss: 3.435\n",
      "[9,    21] loss: 3.465\n",
      "[10,     1] loss: 3.185\n",
      "[10,    11] loss: 3.111\n",
      "[10,    21] loss: 3.237\n",
      "[11,     1] loss: 2.993\n",
      "[11,    11] loss: 3.107\n",
      "[11,    21] loss: 2.973\n",
      "[12,     1] loss: 3.126\n",
      "[12,    11] loss: 3.072\n",
      "[12,    21] loss: 2.802\n",
      "[13,     1] loss: 2.847\n",
      "[13,    11] loss: 2.841\n",
      "[13,    21] loss: 2.917\n",
      "[14,     1] loss: 2.834\n",
      "[14,    11] loss: 2.840\n",
      "[14,    21] loss: 2.762\n",
      "[15,     1] loss: 2.826\n",
      "[15,    11] loss: 2.514\n",
      "[15,    21] loss: 2.683\n",
      "[16,     1] loss: 2.417\n",
      "[16,    11] loss: 2.456\n",
      "[16,    21] loss: 2.450\n",
      "[17,     1] loss: 2.430\n",
      "[17,    11] loss: 2.348\n",
      "[17,    21] loss: 2.452\n",
      "[18,     1] loss: 2.279\n",
      "[18,    11] loss: 1.968\n",
      "[18,    21] loss: 2.235\n",
      "[19,     1] loss: 2.247\n",
      "[19,    11] loss: 2.068\n",
      "[19,    21] loss: 2.188\n",
      "[20,     1] loss: 2.135\n",
      "[20,    11] loss: 1.893\n",
      "[20,    21] loss: 1.996\n",
      "[21,     1] loss: 2.082\n",
      "[21,    11] loss: 2.020\n",
      "[21,    21] loss: 1.884\n",
      "[22,     1] loss: 1.859\n",
      "[22,    11] loss: 1.772\n",
      "[22,    21] loss: 1.873\n",
      "[23,     1] loss: 1.980\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-a898c7ed8aa0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ENABLE_CUDA = False\n",
    "\n",
    "if ENABLE_CUDA:\n",
    "    net.cuda()\n",
    "    \n",
    "loss = None\n",
    "for epoch in range(50):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        if ENABLE_CUDA:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 0:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, loss.item()))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type Net. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(net, 'cnn_original.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.6930147058823529\n"
     ]
    }
   ],
   "source": [
    "train_size = len(train_dataset)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=100)\n",
    "\n",
    "correct = 0\n",
    "for i, (images, labels) in enumerate(train_loader):\n",
    "    if ENABLE_CUDA:\n",
    "        images, labels = images.cuda(), labels.cuda()\n",
    "    results = net(images)\n",
    "    results = results.detach().cpu().numpy()\n",
    "    for i, result in enumerate(results):\n",
    "        if np.argmax(result) == labels[i]:\n",
    "            correct += 1\n",
    "        \n",
    "print(\"accuracy: \", correct/train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.21507352941176472\n"
     ]
    }
   ],
   "source": [
    "test_size = 0\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=100)\n",
    "\n",
    "correct = 0\n",
    "for i, (images, labels) in enumerate(test_loader):\n",
    "    test_size += len(images)\n",
    "    if ENABLE_CUDA:\n",
    "        images, labels = images.cuda(), labels.cuda()\n",
    "    results = net(images)\n",
    "    results = results.detach().cpu().numpy()\n",
    "    for i, result in enumerate(results):\n",
    "        if np.argmax(result) == labels[i]:\n",
    "            correct += 1\n",
    "        \n",
    "print(\"accuracy: \", correct/test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
